{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "4e1d9a8909477db77738c33245c29c7265277ef753467dede8cf3f814cde494e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt   \n",
    "\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'obs_space' : env.observation_space.shape[0],\n",
    "    'action_space' : env.action_space.n,\n",
    "    'hidden_dims' : [24, 48],\n",
    "    'discount_factor' : 1,\n",
    "    'min_exp' : 128,\n",
    "    'max_exp' : 1024,\n",
    "    'batch_size' : 64,\n",
    "    'lr' : 0.02\n",
    "}\n",
    "\n",
    "episodes = 10000\n",
    "epsilon_decay = 0.995\n",
    "epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "copy_step = 10\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.2348, -0.5895]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, obs_space, hidden_dims, action_space):\n",
    "        super(DeepModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(obs_space, hidden_dims[0])\n",
    "        hidden_layers = []\n",
    "        for idx, h_dim in enumerate(hidden_dims[:-1]):\n",
    "            hidden_layers.append(nn.Linear(h_dim, hidden_dims[idx + 1]))\n",
    "            hidden_layers.append(nn.Tanh())\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], action_space)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        h = self.input_layer(xb)\n",
    "        h = self.hidden_layers(h)\n",
    "        out = self.output_layer(h)\n",
    "        return out\n",
    "\n",
    "if debug:\n",
    "    model = DeepModel(env.observation_space.shape[0], [32,32], env.action_space.n)\n",
    "    preds = model(torch.atleast_2d(torch.tensor(env.observation_space.sample())))\n",
    "    print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, obs_space, action_space, hidden_dims, discount_factor, min_exp, max_exp, batch_size, lr):\n",
    "        self.action_space = action_space\n",
    "        self.discount_factor = discount_factor\n",
    "        self.min_exp = min_exp\n",
    "        self.max_exp = max_exp\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.model = DeepModel(obs_space, hidden_dims, action_space)\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr, weight_decay=weight_decay)\n",
    "        self.experience = {\n",
    "            's' : [],\n",
    "            'a' : [],\n",
    "            'r' : [],\n",
    "            's2' : [],\n",
    "            'done' : []\n",
    "        }\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        self.model.eval()\n",
    "        return self.model(torch.tensor(np.atleast_2d(inputs.astype('float32'))))\n",
    "\n",
    "    def train(self, TargetNet):\n",
    "        if len(self.experience['s']) < self.min_exp:\n",
    "            return\n",
    "        \n",
    "        ids = np.random.randint(low = 0, high=len(self.experience['s']), size=self.batch_size)\n",
    "\n",
    "        states = np.asarray(self.experience['s'][ids])\n",
    "        actions = np.asarray(self.experience['a'][ids])\n",
    "        rewards = np.asarray(self.experience['r'][ids])\n",
    "\n",
    "        states_next = np.asarray(self.experience['s2'][ids])\n",
    "        dones = np.asarray(self.experience['done'][ids])\n",
    "        value_next = TargetNet.predict(states_next)\n",
    "        actual_values = np.where(dones, rewards, rewards + self.discount_factor * value_next * np.one_hot(actions,num_classes=self.action_space))\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        selected_action_values = self.predict(states)\n",
    "        loss = nn.MSELoss(rewards, torch.tensor(actual_values))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        if np.random.normal() < epsilon:\n",
    "            return np.random.randint(self.action_space)\n",
    "        else:\n",
    "            predictions = self.predict(state).detach().cpu().numpy()\n",
    "            return np.argmax(predictions)\n",
    "    \n",
    "    def add_experience(self, exp):\n",
    "        if len(self.experience['s']) > self.max_exp:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "                self.experience[key].append(exp[key])\n",
    "    \n",
    "    def copy_weights(self, TargetNet : nn.Module):\n",
    "        self.model.load_state_dict(TargetNet.model.state_dict())\n",
    "    \n",
    "if debug:\n",
    "    agent = DQN(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet : DQN, TargetNet : DQN, epsilon, render=False):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # if render:\n",
    "        #     env.render()\n",
    "        iter += 1\n",
    "\n",
    "        action = TrainNet.get_action(observation, epsilon)\n",
    "        prev_observation = observation\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        if iter == 195:\n",
    "            reward = 100\n",
    "            done = True\n",
    "        else:\n",
    "            reward = iter/200\n",
    "        rewards += reward\n",
    "\n",
    "        exp = {\n",
    "            's' : prev_observation,\n",
    "            'a' : action,\n",
    "            's2' : observation,\n",
    "            'r' : reward,\n",
    "            'done' : done\n",
    "        }\n",
    "\n",
    "        TrainNet.add_experience(exp)\n",
    "        \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "TrainNet = DQN(**config)\n",
    "TargetNet = DQN(**config)\n",
    "\n",
    "running_reward = np.empty(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]<ipython-input-9-b4def061bdb2>:13: RuntimeWarning: Mean of empty slice.\n",
      "  'avg (100 last) reward' : running_reward[max(0, n - 100) : n].mean(),\n",
      "/home/bartek/.local/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 13%|█▎        | 1295/10000 [00:15<02:14, 64.77it/s, episode reward=6.12, avg (100 last) reward=14.1, epsilon=0.01]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm.tqdm(range(episodes))\n",
    "for n in pbar:\n",
    "    if n > 500 and n%10 == 0:\n",
    "        running_reward[n] = play_game(env, TrainNet, TargetNet, epsilon, render=True)\n",
    "    else:\n",
    "        running_reward[n] = play_game(env, TrainNet, TargetNet, epsilon)\n",
    "    TrainNet.train(TargetNet)\n",
    "\n",
    "    if n % copy_step == 0:\n",
    "        TrainNet.copy_weights(TargetNet)\n",
    "    pbar.set_postfix({\n",
    "        'episode reward' : running_reward[n],\n",
    "        'avg (100 last) reward' : running_reward[max(0, n - 100) : n].mean(),\n",
    "        'epsilon' : epsilon\n",
    "    })\n",
    "    epsilon *= epsilon_decay\n",
    "    epsilon = max(epsilon, min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}